{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz: Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('fnm').text\n",
    "        data[\"snm\"] = author.find('snm').text\n",
    "        data[\"email\"] = author.find('email').text       \n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    \n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz Handling Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
    "# and add them to the list for the dictionary key \"insr\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # All data surpport xpath format\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text \n",
    "        # This part is used for attribute name value pairs      \n",
    "        insr = author.findall('./insr')\n",
    "        for i in insr:\n",
    "            data[\"insr\"].append(i.attrib['iid'])  \n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz Using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        inputs = soup.find_all('input')\n",
    "        data[\"eventvalidation\"] = inputs[14]['value']\n",
    "        data[\"viewstate\"] = inputs[13]['value']\n",
    "\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open('options.html', \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id = 'CarrierList')\n",
    "\n",
    "        for i in carrier_list.find_all('option'):\n",
    "            data.append(i['value'])\n",
    "    data=data[3:]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = requests.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['FL',\n 'AS',\n 'AA',\n 'MQ',\n '5Y',\n 'DL',\n 'EV',\n 'F9',\n 'HA',\n 'B6',\n 'OO',\n 'WN',\n 'NK',\n 'US',\n 'UA',\n 'VX']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quiz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "\n",
    "    with open('options.html', \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        airport_list = soup.find(id = 'AirportList')\n",
    "\n",
    "        for i in airport_list.find_all('option'):\n",
    "            data.append(i['value'])\n",
    "    data=data[2:]\n",
    "    data.pop(data.index('AllOthers'))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request and download data\n",
    "import os \n",
    "import requests\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open('options.html', \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id = 'CarrierList')\n",
    "\n",
    "        for i in carrier_list.find_all('option'):\n",
    "            data.append(i['value'])\n",
    "    data=data[3:]\n",
    "\n",
    "    return data\n",
    "    \n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "\n",
    "    with open('options.html', \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        airport_list = soup.find(id = 'AirportList')\n",
    "\n",
    "        for i in airport_list.find_all('option'):\n",
    "            data.append(i['value'])\n",
    "    data=data[2:]\n",
    "    data.pop(data.index('AllOthers'))\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_data(page, carriers, airports):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        data[\"eventvalidation\"] = soup.find(id = '__EVENTVALIDATION')['value']\n",
    "        data[\"viewstate\"] = soup.find(id = '__VIEWSTATE')['value']\n",
    "        data[\"eventvalidation\"] = soup.find(id = '__EVENTVALIDATION')['value']\n",
    "        data[\"viewstategenerator\"] = soup.find(id = '__VIEWSTATEGENERATOR')['value']\n",
    "\n",
    "    data[\"carrier\"] = carriers\n",
    "    data[\"airport\"] = airports\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data,path):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    viewstategenerator = data[\"viewstategenerator\"]\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    for i in range(len(airport)):\n",
    "\n",
    "        r = requests.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                data = ((\"__EVENTTARGET\", \"\"),\n",
    "                        (\"__EVENTARGUMENT\", \"\"),\n",
    "                        (\"__VIEWSTATE\", viewstate),\n",
    "                        (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                        (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                        (\"CarrierList\", carrier[i]),\n",
    "                        (\"AirportList\", airport[i]),\n",
    "                        (\"Submit\", \"Submit\")))\n",
    "        os.chdir(path)\n",
    "        file = open(\"{}.html\".format('%s-%s'%(airport[i],carrier[i])), \"w\")\n",
    "        file.write(r.text)\n",
    "        file.close()\n",
    "\n",
    "    return r.text\n",
    "\n",
    "os.chdir('/Users/rover/Desktop/MongoDB/Lesson2')\n",
    "carriers = extract_carriers('Data Elements.html')\n",
    "airports = extract_airports('Data Elements.html')\n",
    "input_data = extract_data(\"Data Elements.html\", carriers, airports)\n",
    "test = make_request(input_data, '/Users/rover/Desktop/MongoDB/Lesson2/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Running a simple test...\n"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-306ed33be527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-306ed33be527>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Test will loop over three data files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m399\u001b[0m  \u001b[0;31m# Total number of rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-306ed33be527>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"courier\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"airport\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Note: create a new dictionary for each entry in the output data list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# If you use the info dictionary defined here each element in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print (\"Running a simple test...\")\n",
    "    open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "    assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print (\"... success!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'patent.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ca31fd1405d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mget_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-ca31fd1405d6>\u001b[0m in \u001b[0;36mget_root\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \"\"\"\n\u001b[1;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'patent.data'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This and the following exercise are using US Patent database. The patent.data\n",
    "file is a small excerpt of much larger datafiles that are available for\n",
    "download from US Patent website. These files are pretty large ( >100 MB each).\n",
    "The original file is ~600MB large, you might not be able to open it in a text\n",
    "editor.\n",
    "\n",
    "The data itself is in XML, however there is a problem with how it's formatted.\n",
    "Please run this script and observe the error. Then find the line that is\n",
    "causing the error. You can do that by just looking at the datafile in the web\n",
    "UI, or programmatically. For quiz purposes it does not matter, but as an\n",
    "exercise we suggest that you try to do it programmatically.\n",
    "\n",
    "NOTE: You do not need to correct the error - for now, just find where the error\n",
    "is occurring.\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "get_root(PATENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    with open(filename) as xml:\n",
    "        sign = xml.readline()\n",
    "        idx = []\n",
    "        idx.append(0)\n",
    "        index = 0\n",
    "        for j in xml.readlines():\n",
    "            content.append(j)\n",
    "            if j == sign: idx.append(index)\n",
    "            index += 1\n",
    "\n",
    "    for i in range(len(idx)-1):\n",
    "        portion = content[idx[i]:idx[i+1]]\n",
    "        with open(\"{}-{}\".format(filename, i), 'w') as f:\n",
    "            if i == 0:\n",
    "                f.write(\"%s\\n\" % sign)\n",
    "            for item in portion:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "    portion = content[idx[-1]:]\n",
    "    with open(\"{}-{}\".format(filename, i+1), 'w') as f:\n",
    "        for item in portion:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print (\"You have not split the file {} in the correct boundary!\".format(fname))\n",
    "            f.close()\n",
    "        except:\n",
    "            print (\"Could not find file {}. Check if the filename is correct!\".format(fname))\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('patent.data') as xml:\n",
    "    sign = xml.readline()\n",
    "    idx = []\n",
    "    idx.append(0)\n",
    "    index = 0\n",
    "    for j in xml.readlines():\n",
    "        content.append(j)\n",
    "        if j == sign: idx.append(index)\n",
    "        index += 1\n",
    "\n",
    "for i in range(len(idx)-1):\n",
    "    portion = content[idx[i]:idx[i+1]]\n",
    "    with open(\"{}-{}\".format('patent.data', i), 'w') as f:\n",
    "        for item in portion:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "/document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>D 1199</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00009\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>6840196</doc-number>\\n',\n '<kind>B2</kind>\\n',\n '<name>Kirch</name>\\n',\n '<date>20050100</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00010\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>7194981</doc-number>\\n',\n '<kind>B2</kind>\\n',\n '<name>Kirch et al.</name>\\n',\n '<date>20070300</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00011\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>D608060</doc-number>\\n',\n '<kind>S</kind>\\n',\n '<name>Stamos et al.</name>\\n',\n '<date>20100100</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>D30160</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00012\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>7691426</doc-number>\\n',\n '<kind>B2</kind>\\n',\n '<name>Axelrod et al.</name>\\n',\n '<date>20100400</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>426132</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00013\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>D632212</doc-number>\\n',\n '<kind>S</kind>\\n',\n '<name>DiStefano</name>\\n',\n '<date>20110200</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>D11157</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00014\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>8074609</doc-number>\\n',\n '<kind>B2</kind>\\n',\n '<name>Adkins</name>\\n',\n '<date>20111200</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00015\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>D658348</doc-number>\\n',\n '<kind>S</kind>\\n',\n '<name>McCleary</name>\\n',\n '<date>20120500</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>D 1199</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00016\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2005/0271775</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Kirch et al.</name>\\n',\n '<date>20051200</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>426104</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00017\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2009/0004338</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Anderson et al.</name>\\n',\n '<date>20090100</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>426 92</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00018\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2011/0262587</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Stern et al.</name>\\n',\n '<date>20111000</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>426  5</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00019\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2012/0079992</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Chen et al.</name>\\n',\n '<date>20120400</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00020\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2012/0082762</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Weinberg et al.</name>\\n',\n '<date>20120400</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>426 92</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00021\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2012/0085296</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Stern</name>\\n',\n '<date>20120400</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00022\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2012/0186535</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Tu</name>\\n',\n '<date>20120700</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00023\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2013/0047931</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Lai</name>\\n',\n '<date>20130200</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00024\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2013/0047932</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Salmon Hyder et al.</name>\\n',\n '<date>20130200</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<patcit num=\"00025\">\\n',\n '<document-id>\\n',\n '<country>US</country>\\n',\n '<doc-number>2013/0133588</doc-number>\\n',\n '<kind>A1</kind>\\n',\n '<name>Chen et al.</name>\\n',\n '<date>20130500</date>\\n',\n '</document-id>\\n',\n '</patcit>\\n',\n '<category>cited by examiner</category>\\n',\n '<classification-national><country>US</country><main-classification>119710</main-classification></classification-national>\\n',\n '</us-citation>\\n',\n '<us-citation>\\n',\n '<nplcit num=\"00026\">\\n',\n '<othercit>Dingo Mini Rawhide Chews Nov. 20, 2006. [online.] [retrieved on Jul. 11, 2013]. Retrieved from internet &#x3c;URL: http://www.petsmart.com/product/index.jsp?productId=2750713&#x3e;.</othercit>\\n',\n '</nplcit>\\n',\n '<category>cited by examiner</category>\\n',\n '</us-citation>\\n',\n '</us-references-cited>\\n',\n '<number-of-claims>1</number-of-claims>\\n',\n '<us-exemplary-claim>1</us-exemplary-claim>\\n',\n '<us-field-of-classification-search>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D 1100-130</main-classification>\\n',\n '<additional-info>unstructured</additional-info>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D 1199</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426  5</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426 76</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426 87</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426 92</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426 94</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426 95</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426 99</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426101</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426103</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426104</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426108</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426138-139</main-classification>\\n',\n '<additional-info>unstructured</additional-info>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426143</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426144</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426279</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426282</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426283</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426391</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426514</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426549</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426559</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426560</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426660</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>426808</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D21385</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D21386</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D30160</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D11 90</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>D11157</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification> 40310</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>119710</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2 60</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2 98</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2116</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2123-124</main-classification>\\n',\n '<additional-info>unstructured</additional-info>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2127</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2129</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2136</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  21411</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>  2155</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>249117</main-classification>\\n',\n '</classification-national>\\n',\n '<classification-national>\\n',\n '<country>US</country>\\n',\n '<main-classification>249137</main-classification>\\n',\n '</classification-national>\\n',\n '</us-field-of-classification-search>\\n',\n '<figures>\\n',\n '<number-of-drawing-sheets>5</number-of-drawing-sheets>\\n',\n '<number-of-figures>7</number-of-figures>\\n',\n '</figures>\\n',\n '<us-parties>\\n',\n '<us-applicants>\\n',\n '<us-applicant sequence=\"001\" app-type=\"applicant\" designation=\"us-only\">\\n',\n '<addressbook>\\n',\n '<last-name>Chen</last-name>\\n',\n '<first-name>Zuxi</first-name>\\n',\n '<address>\\n',\n '<city>Wenzhou</city>\\n',\n '<country>CN</country>\\n',\n '</address>\\n',\n '</addressbook>\\n',\n '<residence>\\n',\n '<country>CN</country>\\n',\n '</residence>\\n',\n '</us-applicant>\\n',\n '</us-applicants>\\n',\n '<inventors>\\n',\n '<inventor sequence=\"001\" designation=\"us-only\">\\n',\n '<addressbook>\\n',\n '<last-name>Chen</last-name>\\n',\n '<first-name>Zuxi</first-name>\\n',\n '<address>\\n',\n '<city>Wenzhou</city>\\n',\n '<country>CN</country>\\n',\n '</address>\\n',\n '</addressbook>\\n',\n '</inventor>\\n',\n '</inventors>\\n',\n '<agents>\\n',\n '<agent sequence=\"01\" rep-type=\"attorney\">\\n',\n '<addressbook>\\n',\n '<orgname>Davidson, Davidson &#x26; Kappen, LLC</orgname>\\n',\n '<address>\\n',\n '<country>unknown</country>\\n',\n '</address>\\n',\n '</addressbook>\\n',\n '</agent>\\n',\n '</agents>\\n',\n '</us-parties>\\n',\n '<assignees>\\n',\n '<assignee>\\n',\n '<addressbook>\\n',\n '<orgname>Wenzhou Yuxiang Pet Product Co., Ltd.</orgname>\\n',\n '<role>03</role>\\n',\n '<address>\\n',\n '<city>Shuitou Town Pingyang County Zhejiang P</city>\\n',\n '<country>CN</country>\\n',\n '</address>\\n',\n '</addressbook>\\n',\n '</assignee>\\n',\n '</assignees>\\n',\n '<examiners>\\n',\n '<primary-examiner>\\n',\n '<last-name>Brooks</last-name>\\n',\n '<first-name>Cathron</first-name>\\n',\n '<department>2911</department>\\n',\n '</primary-examiner>\\n',\n '<assistant-examiner>\\n',\n '<last-name>Mroczka</last-name>\\n',\n '<first-name>Katie</first-name>\\n',\n '</assistant-examiner>\\n',\n '</examiners>\\n',\n '</us-bibliographic-data-grant>\\n',\n '<drawings id=\"DRAWINGS\">\\n',\n '<figure id=\"Fig-EMI-D00000\" num=\"00000\">\\n',\n '<img id=\"EMI-D00000\" he=\"116.16mm\" wi=\"166.62mm\" file=\"USD0696836-20140107-D00000.TIF\" alt=\"embedded image\" img-content=\"drawing\" img-format=\"tif\"/>\\n',\n '</figure>\\n',\n '<figure id=\"Fig-EMI-D00001\" num=\"00001\">\\n',\n '<img id=\"EMI-D00001\" he=\"177.29mm\" wi=\"141.82mm\" orientation=\"landscape\" file=\"USD0696836-20140107-D00001.TIF\" alt=\"embedded image\" img-content=\"drawing\" img-format=\"tif\"/>\\n',\n '</figure>\\n',\n '<figure id=\"Fig-EMI-D00002\" num=\"00002\">\\n',\n '<img id=\"EMI-D00002\" he=\"210.90mm\" wi=\"163.83mm\" orientation=\"landscape\" file=\"USD0696836-20140107-D00002.TIF\" alt=\"embedded image\" img-content=\"drawing\" img-format=\"tif\"/>\\n',\n '</figure>\\n',\n '<figure id=\"Fig-EMI-D00003\" num=\"00003\">\\n',\n '<img id=\"EMI-D00003\" he=\"197.70mm\" wi=\"137.41mm\" file=\"USD0696836-20140107-D00003.TIF\" alt=\"embedded image\" img-content=\"drawing\" img-format=\"tif\"/>\\n',\n '</figure>\\n',\n '<figure id=\"Fig-EMI-D00004\" num=\"00004\">\\n',\n '<img id=\"EMI-D00004\" he=\"231.31mm\" wi=\"127.34mm\" orientation=\"landscape\" file=\"USD0696836-20140107-D00004.TIF\" alt=\"embedded image\" img-content=\"drawing\" img-format=\"tif\"/>\\n',\n '</figure>\\n',\n '<figure id=\"Fig-EMI-D00005\" num=\"00005\">\\n',\n '<img id=\"EMI-D00005\" he=\"228.01mm\" wi=\"139.70mm\" orientation=\"landscape\" file=\"USD0696836-20140107-D00005.TIF\" alt=\"embedded image\" img-content=\"drawing\" img-format=\"tif\"/>\\n',\n '</figure>\\n',\n '</drawings>\\n',\n '<description id=\"description\">\\n',\n '<?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"lead\"?>\\n',\n '<description-of-drawings>\\n',\n '<p id=\"p-0001\" num=\"0001\"><figref idref=\"DRAWINGS\">FIG. 1</figref> is a perspective view of the dog chew;</p>\\n',\n '<p id=\"p-0002\" num=\"0002\"><figref idref=\"DRAWINGS\">FIG. 2</figref> is a first plan view of the dog chew;</p>\\n',\n '<p id=\"p-0003\" num=\"0003\"><figref idref=\"DRAWINGS\">FIG. 3</figref> is a second plan view of the dog chew;</p>\\n',\n '<p id=\"p-0004\" num=\"0004\"><figref idref=\"DRAWINGS\">FIG. 4</figref> is a first side view of the dog chew;</p>\\n',\n '<p id=\"p-0005\" num=\"0005\"><figref idref=\"DRAWINGS\">FIG. 5</figref> is a second side view of the dog chew;</p>\\n',\n '<p id=\"p-0006\" num=\"0006\"><figref idref=\"DRAWINGS\">FIG. 6</figref> is a third plan view of the dog chew; and,</p>\\n',\n '<p id=\"p-0007\" num=\"0007\"><figref idref=\"DRAWINGS\">FIG. 7</figref> is a fourth plan view of the dog chew.</p>\\n',\n '<p id=\"p-0008\" num=\"0008\">The smaller stippled shading and larger stippled shading shown in <figref idref=\"DRAWINGS\">FIGS. 1 to 7</figref> represent a contrasting appearance.</p>\\n',\n '</description-of-drawings>\\n',\n '<?brief-description-of-drawings description=\"Brief Description of Drawings\" end=\"tail\"?>\\n',\n '</description>\\n',\n '<us-claim-statement>CLAIM</us-claim-statement>\\n',\n '<claims id=\"claims\">\\n',\n '<claim id=\"CLM-00001\" num=\"00001\">\\n',\n '<claim-text>The ornamental design for the dog chew, as shown and described.</claim-text>\\n',\n '</claim>\\n',\n '</claims>\\n',\n '</us-patent-grant>\\n']"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}